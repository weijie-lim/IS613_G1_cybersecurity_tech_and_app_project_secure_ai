# IS613_G1_cybersecurity_tech_and_app_project_secure_ai
This is a course work project specifically looking at what are one of the ways AI models can be attacked how we are able to secure these models.

Topics discussed:
1. Practical discussion of an actual possible occurrence of how backdoor and poisoning attacks can affect real-world systems.
2. Explanation of FGSM
3. Detection against FGSM using the IBM adverserial robustness toolbox


Credit to the following authors for their contribution towards securing AI: 
</br> Alexander Madry https://github.com/MadryLab/mnist_challenge 
</br> Resources from IBM https://github.com/IBM/Trusted-ML-Pipelines & https://github.com/Trusted-AI/adversarial-robustness-toolbox
