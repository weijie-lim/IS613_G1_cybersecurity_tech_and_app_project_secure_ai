# IS613_G1_cybersecurity_tech_and_app_project_secure_ai
This is a course work project specifically looking at what are one of the ways AI models can be attacked how we are able to secure these models.

Topics discussed in the ipynb notebooks:
1. Practical discussion of an actual possible occurrence of how backdoor and poisoning attacks can affect real-world systems.
2. Explanation of FGSM
  - <a> https://tinyurl.com/FGSM-eg </a>
3. Detection against FGSM using the IBM adverserial robustness toolbox


Credit and References: 
</br> 1. Alexander Madry https://github.com/MadryLab/mnist_challenge 
</br> 2. Resources from IBM https://github.com/IBM/Trusted-ML-Pipelines & https://github.com/Trusted-AI/adversarial-robustness-toolbox
</br> 3. DEEPSEC: https://github.com/ryderling/DEEPSEC
