# IS613_G1_cybersecurity_tech_and_app_project_secure_ai
This is a course work project specifically looking at what are one of the ways AI models can be attacked how we are able to secure these models.

Topics discussed in the ipynb notebooks:
1. Practical discussion of an actual possible occurrence of how backdoor and poisoning attacks can affect real-world systems.
2. Explanation of FGSM
  - <a> https://tinyurl.com/FGSM-eg </a>
3. Detection against FGSM using the IBM adverserial robustness toolbox
4. A Different Example on MNIST / Cifar dataset using DEEPSEC's resources
  - BIM: Improvements to FGSM Attack
  - Evaluating Attack Metrics
  - Preparing Defenses Against such Attacks
  - Evaluating Defense Metrics


Credit and References: 
</br> 1. Alexander Madry https://github.com/MadryLab/mnist_challenge 
</br> 2. Resources from IBM https://github.com/IBM/Trusted-ML-Pipelines & https://github.com/Trusted-AI/adversarial-robustness-toolbox
</br> 3. DEEPSEC: https://github.com/ryderling/DEEPSEC
</br> 4. MNIST Data Set: https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=mnist & https://yann.lecun.com/exdb/mnist/
</br> 5. Cifar Data Set: https://www.cs.toronto.edu/~kriz/cifar.html
